---
title: "Final Assignment"
author: "Alp Ã–nder Yener"
date: "2023-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notice: The stan codes on this file have a blank line at the end but when compiled, the blank line seems to disappear. 

## Question 1

### 1.1

First, setup the data <br>
Then, to find a posterior for theta, compute likelihood and prior <br>
Use the formula:
posterior = prior * likelihood (for unnormalised posterior)

```{r aplot, message=FALSE}
N = 50
x = 7
theta = 0.5

prior = dbeta(theta, 1,2)
likelihood = dbinom(x = x, size = N, prob = theta)

posterior = likelihood*prior
print(posterior)
```

To elaborate, I used the formula:
$P(\theta | D) = P(D | \theta)P(\theta)$

### 1.2

We have to sum for a specified alpha value, over all beta values.
If the distribution was continuous, this would be equal to integrating over $d\beta$

Formula (discrete):
$P(\alpha) = \sum_{\beta} P(\alpha, \beta)$

Code-wise, we can do this for example:
Suppose we have a dataframe with alpha and beta values


```{r qplot, message=FALSE}
library(magrittr)
library(tidyverse)

alpha = c(1,1,2,2)
beta = c(0,1,2,3)
prob = c(0.25,0.3,0.3,0.15)

df = data.frame(alpha = alpha, beta = beta, prob = prob)

marginal_alpha <- df %>% 
  group_by(alpha) %>% 
  summarize(prob = sum(prob))
  
print(marginal_alpha)
```


### 1.3

We can modify the formula in 1.2 as follows: <br>
$P(\alpha < \beta) = \sum_{i=1}^{N} P(\alpha = \alpha_i < \beta = \beta_i)$

Codewise... (for example)

```{r bplot, message=FALSE}
library(magrittr)
library(tidyverse)

alpha = c(1,1,2,2)
beta = c(0,1,2,3)
prob = c(0.25,0.3,0.3,0.15)

df = data.frame(alpha = alpha, beta = beta, prob = prob)

alpha_smaller <- df %>% 
  group_by(alpha<beta) %>% 
  summarize(prob = sum(prob))

print(alpha_smaller)
```
### 1.4

When we already have posterior samples, we can use those values <br>

Then we get random picks from the distribution, given the sample size and the
samples for the chosen parameter

The skeleton of the R code would look something like this:

```
y_tilda = r{dist}(n=sample_size, {any other parameters needed},
                 {chosen_parameter} = posterior_samples)
```
### 1.5

If that is the case, then too many proposals are rejected. <br>
According to the algorithm, this may be because r is too low hence the proposal hardly gets accepted.

The proposal distribution proposes the next point, for the algorithm to calculate r. If r is always too low, this implies that the proposal distribution does not give good proposals.

In other words, it gives proposals that are not suitable for our target distribution.

### 1.6
During the early phase, the algorithm is still "warming up" which means that it is still trying to find the probabilistic hot spot for the target distribution.

In other words, it is still trying to find where to converge.

During this search for the hot spot, the algorithm will "wander around and explore" which will make the first portion of the iterations useless for the result.

### 1.7
What the question says: $\psi \sim \mathcal{N}(0,1)$ <br>

To make it hierarchical, we can introduce new parameters: <br>
$\psi \sim \mathcal{N}(\psi_{\mu}, \psi_{\sigma})$ <br>
$\psi_{\mu} \sim \mathcal{N}(0, 1)$ <br>
$\psi_{\sigma} \sim Gamma(2, 1)$

### 1.8
AIC is a measurement of how well a model fits the data it was generated from. <br>
The smaller the AIC, the better.

WAIC is the Bayesian version of AIC.
WAIC also works for hierarchical models.

AIC is not Bayesian because no posterior or prior information is involved.













### 1.9

The squared exponential covariance kernel has 2 hyperparameters.

1) The length-scale. It affects the waves of the Gaussian Process.
Increasing the length-scale results in more subtle, wide waves, while decreasing
it results in more "wiggly", more "chaotic" waves. <br>
This is because when the length-scale is low, the x values that are considered
similar are more close to each other. Hence, the y values generated are similar
only for very close x values. The opposite applies for big length-scale values. <br>
Hence, it controls how the functions generated by a Gaussian Process
varies in the x-direction.


2) Variance. It controls the average distance away from the mean.
Increasing the variance results in longer distances between the low and high
points on the waves, while decreasing does the opposite. It simply controls the
height of the waves. <br>
Hence, it controls how the functions generated by a Gaussian Process
varies in the y-direction.








### 1.10
The file does not end with a blank line. When a blank line is added, it works.

Fixed stan code: (added a blank line at the end)
```
data {
  int<lower=0> N;
  vector[N] x;
}
parameters {
  real phi; 
  real sigma;
}
model {
  
  for (i in 2:N)
    x[i] ~ normal(phi * x[i-1], sigma);

  phi ~ normal(0, 1);
  sigma ~ normal(0, 1); 
}


```




## Question 2

Models used:
$y \sim \mathcal{N}(a + bx,1)$ <br>
$a \sim \mathcal{N}(2,3)$ <br>
$b \sim \mathcal{N}(1.5,1.5)$

```{r q2plot, message=FALSE, cache=TRUE}
# Setup (copied from the exercise solutions)

library(tidyverse)
library(magrittr)
library(rstan)
library(grafify)
theme_set(theme_bw(20))
okabi_colors <- c("#E69F00", "#56B4E9", "#009E73")
prior_color <- "#009E73"
likelihood_color <- "#E69F00"
posterior_color <- "#56B4E9"

# Data:
x = c(0.27,0.12,-0.04,-1.12,-1.35,0.77,0.41,-0.35,-0.11,0.82)
y = c(2.06,4.3,3.86,1.02,0.16,3.97,3.32,1.93,3.22,2.85)

# Define grids
delta = 0.01
a_grid = seq(from = -1, to = 5, by = delta)
b_grid = seq(from = 0, to = 3, by = delta)

# Create df
df = expand.grid(a = a_grid, b = b_grid)

# Likelihood
for(i in 1:nrow(df)) {
  
  df[i, "likelihood"] = prod(dnorm(x = y,
                                    mean = df[i, "a"] + df[i, "b"] * x,
                                    sd = 1))
}

# Prior, posterior and normalization
# Due to the coefficient delta^2, sum(df$posterior) = 10000
# Hence some values on the graph are large
df <- df %>% 
  mutate(prior = dnorm(a, 2, 3) * dnorm(b, 1.5, 1.5), 
         posterior_unnormalize = prior*likelihood, 
         posterior = posterior_unnormalize/(sum(posterior_unnormalize)*delta^2))

# Find the MAP
posterior_mode <- df[which.max(df$posterior), c("a", "b")]
print(posterior_mode)

# Plot the full posterior and the MAP lines
p <- df %>% 
  ggplot(aes(x = a, y = b, fill = posterior)) +
  geom_tile() +
  scale_fill_gradientn(colours = rainbow(5)) +
  ggtitle("Full Posterior with MAPs") +
  geom_hline(yintercept = posterior_mode$b, color = "black") +
  geom_vline(xintercept = posterior_mode$a, color = "black")


p

```


## Question 3
### Stan code:
```{stan, output.var="final_q3.stan", cache=TRUE, results="hide"}
data {
  // Input
  int<lower=0> N;
  vector[N] y;
  vector[N] x;
}
parameters {
  real b_1; 
  real b_2;
  real b_3;
  real sigma;
}
model {
  // Likelihood
  y ~ normal(b_1 + b_2*x + b_3*(x^2), sigma^2);
  
  // Priors
  b_1 ~ normal(0,1);
  b_2 ~ normal(0,1);
  b_3 ~ normal(0,1);
  sigma ~ normal(0,1);

}


```

### R code:
```{r q3plot, message=FALSE, cache=TRUE, warning=FALSE, results='hide'}
# Setup (copied from the exercise solutions)

library(tidyverse)
library(magrittr)
library(rstan)
library(grafify)
theme_set(theme_bw(20))
okabi_colors <- c("#E69F00", "#56B4E9", "#009E73")
prior_color <- "#009E73"
likelihood_color <- "#E69F00"
posterior_color <- "#56B4E9"



# Generate 20 samples

y = c()
x = c()

for (i in 1:20){
  x_new = rnorm(1,0,1)
  x = append(x, x_new)
  new_y = rnorm(1, 1 + 2*x_new + 1.2*(x_new^2), 1)
  y = append(y, new_y)
}

# Start and fit the model

normal_model = stan_model("final_q3.stan")

normal_fit <- sampling(normal_model,
                        list(N = length(y),
                             y = y, 
                             x = x))

# Get samples
df = data.frame(b_1 = extract(normal_fit, "b_1")[[1]],
                b_2 = extract(normal_fit, "b_2")[[1]],
                b_3 = extract(normal_fit, "b_3")[[1]],
                sigma = extract(normal_fit, "sigma")[[1]])

# Draw histograms for each parameter to show that
# the parameters are recovered approximately
ggplot() +
geom_histogram(data = data.frame(b_1 = df$b_1), 
               aes(b_1), bins = 75) +
  geom_vline(xintercept = 1, color = "blue") +
  ggtitle("b_1 samples")

ggplot() +
  geom_histogram(data = data.frame(b_2 = df$b_2), 
                 aes(b_2), bins = 75) +
  geom_vline(xintercept = 2, color = "blue") +
  ggtitle("b_2 samples")

ggplot() +
  geom_histogram(data = data.frame(b_3 = df$b_3), 
                 aes(b_3), bins = 75) +
  geom_vline(xintercept = 1.2, color = "blue") +
  ggtitle("b_3 samples")

ggplot() +
  geom_histogram(data = data.frame(sigma = df$sigma), 
                 aes(sigma), bins = 75) +
  geom_vline(xintercept = 1, color = "blue") +
  ggtitle("sigma samples")

# Since -1 and 1 are the same when you square it, the sigma graph makes sense



ggplot() +
  geom_histogram(data = data.frame(sigma = abs(df$sigma)), 
                 aes(sigma), bins = 75) +
  geom_vline(xintercept = 1, color = "blue") +
  ggtitle("sigma samples, in absolute value")

```
<br>
In $ax^2 + bx + c = 0$ if $b^2 - 4ac < 0$, then the equation does not have any real roots

In our formula, this corresponds to: $(b_2)^2 - 4 \cdot b_3 \cdot b_1 < 0$, which corresponds to
$(b_2)^2 < 4 \cdot b_3 \cdot b_1$

```{r q3plot_2}
no_real_roots_prob = mean((df$b_2)^2 < 4*df$b_3*df$b_1)
print(no_real_roots_prob)
```





## Question 4
### Stan code:
```{stan, output.var="final_q4.stan", cache=TRUE}
data {
  // Input
  int<lower=0> N;
  array[N] int y;
  vector[N] x;
}
parameters {
  real alpha;
  real beta;
}
transformed parameters{
  // Define theta from alpha, beta and x
  vector<lower=0, upper=1>[N] theta;
  
  for (n in 1:N) {
  theta[n] = 1/ (1 + exp(-(alpha + beta * x[n])));
}
}
model {
  // Likelihood
  y ~ bernoulli(theta);
  
  // Priors
  alpha ~ normal(-4, 2);
  beta ~ normal(1,2);

}




```

### R code:
```{r q4plot, message=FALSE, cache=TRUE, warning=FALSE, results='hide'}

# Setup (copied from the exercise solutions)

library(tidyverse)
library(magrittr)
library(rstan)
library(grafify)
theme_set(theme_bw(20))
okabi_colors <- c("#E69F00", "#56B4E9", "#009E73")
prior_color <- "#009E73"
likelihood_color <- "#E69F00"
posterior_color <- "#56B4E9"


# Generate data

y = c()
x = c()
theta = c()
alpha = -5
beta = 1.5

for (i in 1:46){
  x_new = runif(1,0,10)
  x = append(x, x_new)
  
  theta_new = 1/ (1 + exp(-(alpha + beta * x_new)))
  theta = append(theta, theta_new)
  
  y_new = rbernoulli(1, theta_new)
  y = append(y, y_new)
}

# Start and fit the model

bernoulli_model = stan_model("final_q4.stan")

bernoulli_fit <- sampling(bernoulli_model,
                       list(N = length(y),
                            y = y, 
                            x = x))

# Get samples
df = data.frame(alpha = extract(bernoulli_fit, "alpha")[[1]],
                beta = extract(bernoulli_fit, "beta")[[1]])

# Draw histograms for each parameter to show that
# the parameters are recovered approximately
ggplot() +
  geom_histogram(data = data.frame(alpha = df$alpha), 
                 aes(alpha), bins = 75) +
  geom_vline(xintercept = -5, color = "blue") +
  ggtitle("alpha samples")

ggplot() +
  geom_histogram(data = data.frame(beta = df$beta), 
                 aes(beta), bins = 75) +
  geom_vline(xintercept = 1.5, color = "blue") +
  ggtitle("beta samples")

```

calculate average theta when x = 5

```{r q4plot_2}
df$theta_when_5hrs = 1/ (1 + exp(-(df$alpha + df$beta * 5)))
print(mean(df$theta_when_5hrs))
```


## Question 6
### Stan code:
```{stan, output.var="final_q6.stan", cache=TRUE, results="hide"}
data {
  int<lower=1> row_num; // number of rows
  int<lower=1> col_num; // number of columns (without the time column)
  matrix[row_num, col_num] x; // observations (P_meas(t))
  vector[row_num] t; // time column
  real<lower=0> P_o; // P_0 for the cultures of bacteria
}
parameters {
  row_vector<lower=0>[col_num] K; // population boundary
  row_vector[col_num] r; // growth rate
  real<lower=0> sigma_sq;
  
  // Hyperparameters
  real mu_r;
  real<lower=0> sigma_r;
}
transformed parameters{
  matrix<lower=0, upper=2>[row_num, col_num] Pt;
  
  for (n in 1:row_num) {
    for (m in 1:col_num) {
    Pt[n, m] = K[m]/(1+((K[m]/P_o)-1)*exp(-r[m]*t[n]));
  }
}  
}
model {
  
  // Likelihood
  for (n in 1:row_num) {
    for (m in 1:col_num) {
    x[n, m] ~ normal(Pt[n,m], sigma_sq);
  }
}  

  // Priors
  K ~ normal(1.5,0.5);
  r ~ normal(mu_r, sigma_r);
  sigma_sq ~ normal(0.005, 0.05);
  
  
  // Hyperpriors
  mu_r ~ normal(0.5, 0.1);
  sigma_r ~ gamma(1, 1);
}
generated quantities {
  real r_tilde = normal_rng(mu_r, sigma_r);
}




```

### R code:
```{r q6plot, message=FALSE, cache=TRUE, warning=FALSE, results='hide'}
# Setup (copied from the exercise solutions)

library(tidyverse)
library(magrittr)
library(rstan)
library(grafify)
theme_set(theme_bw(20))
okabi_colors <- c("#E69F00", "#56B4E9", "#009E73")
prior_color <- "#009E73"
likelihood_color <- "#E69F00"
posterior_color <- "#56B4E9"


# Get the data
path = "C:/Users/alpye/Downloads/ecoli.txt"
x = read.csv(file = path, header = TRUE)
t = x[, 1, drop = FALSE]
t = unlist(t[, 1, drop = FALSE])
x = x[, -1, drop = FALSE] # get the data without time
P_o = 0.005


# Start and fit the model

e_coli_model = stan_model("final_q6.stan")

e_coli_fit <- sampling(e_coli_model,
                          list(row_num = nrow(x),
                               col_num = ncol(x), 
                               x = x,
                               t = t,
                               P_o = P_o))

# Extract r (plus the observed r given in the question)
df = data.frame(r = extract(e_coli_fit, "r")[[1]])
r_vect = c(0.724,0.683,0.423,0.262,0.411,0.829,0.469,0.629,0.667,0.384)

# Plot 10 Histograms and compare the r samples with the given r values

ggplot(df, aes(x = r.1)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[1], color = "red") +
  ggtitle("r1")

ggplot(df, aes(x = r.2)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[2], color = "red") +
  ggtitle("r2")

ggplot(df, aes(x = r.3)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[3], color = "red") +
  ggtitle("r3")

ggplot(df, aes(x = r.4)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[4], color = "red") +
  ggtitle("r4")

ggplot(df, aes(x = r.5)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[5], color = "red") +
  ggtitle("r5")

ggplot(df, aes(x = r.6)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[6], color = "red") +
  ggtitle("r6")

ggplot(df, aes(x = r.7)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[7], color = "red") +
  ggtitle("r7")

ggplot(df, aes(x = r.8)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[8], color = "red") +
  ggtitle("r8")

ggplot(df, aes(x = r.9)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[9], color = "red") +
  ggtitle("r9")

ggplot(df, aes(x = r.10)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = r_vect[10], color = "red") +
  ggtitle("r10")


```

```{r q6plot_2}

df_r = data.frame(r_tilde = rstan::extract(e_coli_fit, "r_tilde")[[1]])
print(mean(df_r$r_tilde < 0))
```

## Question 7
### Stan code:
```{stan, output.var="final_q7.stan", cache=TRUE, results="hide"}
data {
  
  // Input Data
  int<lower=1> N;
  real y[N];
  real x[N];
  
  // Gaussian Process hyperparameters
  real<lower=0> alpha;
  real<lower=0> lambda;
  
}
transformed data {
  matrix[N, N] K;

  // Matern 3/2 covariance kernel
  K = gp_matern32_cov(x, alpha, lambda);
  
  // Add a bit of noise to the diagonal for numerical stability
  for (n in 1:N) {
    K[n, n] = K[n, n] + 1e-6;
  }


}
parameters {
  // time-varying variance function
  vector[N] sigma_log;
}
model {

  // Likelihood
  y ~ normal(0, square(exp(sigma_log)));

  // GP
  sigma_log ~ multi_normal(rep_vector(-1, N), K);

}




```

### R code:
```{r q7plot, message=FALSE, cache=TRUE, warning=FALSE, results='hide'}
# Setup (copied from the code examples for Gaussian Processes)

knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(tidyverse)
library(magrittr)
library(grafify)
theme_set(theme_bw(20))
okabi_colors <- c("#E69F00", "#56B4E9", "#009E73")
prior_color <- "#009E73"
likelihood_color <- "#E69F00"
posterior_color <- "#56B4E9"


# Get the data
path = "C:/Users/alpye/Downloads/EURUSD.txt"
df = read.csv(file = path, header = TRUE)
y = as.array(df$Return_scaled)
x = as.array(df$Time)
alpha = 1
lambda = 30
N = nrow(df)


# Start and fit the model

usd_model = stan_model("final_q7.stan")

usd_fit <- sampling(usd_model,
                       list(N = N,
                            y = y, 
                            x = x,
                            alpha = alpha,
                            lambda = lambda),
                    chains = 1, iter = 1000)



# Got sigma square samples
sigma_sq_samples <- (exp(rstan::extract(usd_fit, "sigma_log")[["sigma_log"]]))^2 %>% 
  t %>% data.frame() %>% 
  mutate(x = df$Time)

sigma_sq_samples_l <- sigma_sq_samples %>% 
  gather(key = "sample", value = "sigma_sq", -x)


# Got the normal sigma samples and the mean
sigma_samples = (exp(rstan::extract(usd_fit, "sigma_log")[["sigma_log"]]))
sigma_mean = mean(sigma_samples)

# Plot
p_f <- ggplot() +
  geom_line(
    data = sigma_sq_samples_l,
    aes(x = x, y = sigma_sq, group = sample),
    alpha = 0.01) +
  geom_point(data = df, 
             aes(x = Time, y = Return_scaled), color ="red") +
  ggtitle("Sigma square values (black lines) and \n Return_scaled values (red dots) 
          and sigma mean (blue line)") +
  geom_hline(yintercept = sigma_mean, color = "blue") +
  labs(y="y")
  

print(p_f)

```
<br>

### Explanation for the graph:

As we can see, the variance increases just after point x=50. <br>
To make blue more visible, I made the black lines' alpha parameter = 0.01
