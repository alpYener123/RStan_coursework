#1.1###########################################################################

# To find a posterior for theta, we need to first compute likelihood and prior

N = 50
x = 7
theta = 0.5

prior = dbeta(theta, 1,2)
likelihood = dbinom(x = x, size = N, prob = theta)

# posterior = prior * likelihood (unnormalised)
posterior = likelihood*prior
print(posterior)

# To explain verbally:
# P(theta | D) = P(D | theta) * P(theta)
# posterior = likelihood * prior


#1.2###########################################################################

# We have to sum for a specified alpha value, over all beta values
# If it was continuous, this would be equal to integrating over d(beta)

# Formula:

# P(alpha) = sum_from_i=1_to_i=N(P(alpha, beta = beta_i))

# Code-wise, we can do this for example:
# Suppose we have a dataframe with alpha and beta values
alpha = c(1,1,2,2)
beta = c(0,1,2,3)
prob = c(0.25,0.3,0.3,0.15)

df = data.frame(alpha = alpha, beta = beta, prob = prob)

marginal_alpha <- df %>% 
  group_by(alpha) %>% 
  summarize(prob = sum(prob))

#1.3###########################################################################

# We can modify the formula in 1.2 as follows:

# P(alpha < beta) = sum_from_i=1_to_i=N(P(alpha=alpha_i < beta=beta_i))

# Codewise... (for example)

alpha = c(1,1,2,2)
beta = c(0,1,2,3)
prob = c(0.25,0.3,0.3,0.15)

df = data.frame(alpha = alpha, beta = beta, prob = prob)

alpha_smaller <- df %>% 
  group_by(alpha<beta) %>% 
  summarize(prob = sum(prob))

#1.4###########################################################################

# When we already have posterior samples, we can use those values
# So, we have all the points in the sample for the chosen parameter

# Then we get random picks from the distribution, given the sample size and the
# samples for the chosen parameter

# The skeleton of the code would look something like this:

# y_tilda = r{dist}(n=sample_size, {any other parameters needed},
#                   {chosen_parameter} = posterior_samples)

#1.5###########################################################################

# If that is the case, then too many proposals are rejected.
# According to the algorithm, this may be because r is too low hence
# the proposal hardly gets accepted.

# The proposal distribution proposes the next point, for the algorithm to
# calculate r. If r is always too low, this implies that
# the proposal distribution does not give good proposals.

# In other words, it gives proposals that are not
# suitable for our target distribution

#1.6###########################################################################

# During the early phase, the algorithm is still "warming up" which means
# that it is still trying to find the probabilistic hot spot for the target distribution.
# In other words, it is still trying to find where to converge.

# During this search for the hot spot, the algorithm will "wander around and
# explore" which will make the first portion of the iterations useless for the result.

#1.7###########################################################################

# What the question says:
# psi ~ N(0,1)

# To make it hierarchical, we can introduce new parameters:
# psi ~ N(psi_mu, psi_var)
# psi_mu ~ N(0,1)
# psi_var ~ Gamma(2,1)

#1.8###########################################################################

# AIC is a measurement of how well a model fits the data it was generated from.
# The smaller the AIC, the better.

# WAIC is the Bayesian version of AIC.
# WAIC also works for hierarchical models.

# AIC is not Bayesian because no posteriors or prior informations are involved.

#1.9###########################################################################

# The squared exponential covariance kernel has 2 hyperparameters.

# 1) The length-scale. It affects the waves of the Gaussian Process.
# Increasing the length-scale results in more subtle, wide waves, while decreasing
# it results in more "wiggly", more "chaotic" waves.

# This is because when the length-scale is low, the x values that are considered
# similar are more close to each other. Hence, the y values generated are similar
# only for very close x values. The opposite applies for big length-scale values.

# Hence, it controls how the functions generated by a Gaussian Process
# varies in the x-direction.


# 2) Variance. It controls the average distance away from the mean.
# Increasing the variance results in longer distances between the low and high
# points on the waves, while decreasing does the opposite. It simply controls the
# height of the waves.

# Hence, it controls how the functions generated by a Gaussian Process
# varies in the y-direction.


#1.10###########################################################################

# The file does not end with a blank line. That is the only problem with the file.

############################################################################
